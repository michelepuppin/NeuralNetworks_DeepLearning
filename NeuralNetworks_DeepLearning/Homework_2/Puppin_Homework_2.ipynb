{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Puppin_Homework_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uyMwREklV9O4"},"source":["#NEURAL NETWORKS AND DEEP LEARNING\n","\n","## Homework 2 - Unsupervised Deep Learning\n","\n","Puppin Michele - 1227474"]},{"cell_type":"code","metadata":{"id":"eS3qYcngVycb"},"source":["# Import packages\n","import matplotlib.pyplot as plt \n","import numpy as np \n","import pandas as pd \n","import random \n","import os \n","from tqdm import tqdm\n","from google.colab import files\n","\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import itertools\n","\n","# Set random seed\n","np.random.seed(25)\n","torch.manual_seed(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rpcncYzXf4R"},"source":["# Set device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Training device: {device}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1kKmqnqXbP-"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"CP8qLCgTXZeS"},"source":["# Download the data and create dataset\n","train_dataset = torchvision.datasets.MNIST('dataset', train=True, download=True)\n","test_dataset  = torchvision.datasets.MNIST('dataset', train=False, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEBPRiezYLwV"},"source":["# Set the train transform\n","train_dataset.transform = transforms.Compose([ transforms.ToTensor(), ])\n","# Set the test transform\n","test_dataset.transform = transforms.Compose([ transforms.ToTensor(), ])\n","\n","# Define train dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n","# Define test dataloader\n","test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPcfjVqWZ21F"},"source":["## Early stopping"]},{"cell_type":"code","metadata":{"id":"EO1LlP_1Z6h_"},"source":["# Early stopping class definition\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1sj3gXoYjOa"},"source":["# Autoencoders Definition"]},{"cell_type":"code","metadata":{"id":"zvHEOfSgYpUA"},"source":["# Encoder definition\n","\n","class Encoder(nn.Module):\n","    \n","    def __init__(self, encoded_space_dim):\n","        super().__init__()\n","        \n","        ### Convolutional section\n","        self.encoder_cnn = nn.Sequential(\n","            # First convolutional layer\n","            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(True),\n","            # Second convolutional layer\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(True),\n","            # Third convolutional layer\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=0),\n","            nn.ReLU(True)\n","        )\n","        \n","        ### Flatten layer\n","        self.flatten = nn.Flatten(start_dim=1)\n","\n","        ### Linear section\n","        self.encoder_lin = nn.Sequential(\n","            # First linear layer\n","            nn.Linear(288, 64),\n","            nn.ReLU(),\n","            # Second linear layer\n","            nn.Linear(64, encoded_space_dim)\n","        )\n","        \n","    def forward(self, x):\n","        # Apply convolutions\n","        x = self.encoder_cnn(x)\n","        # Flatten\n","        x = self.flatten(x) \n","        # Apply linear layers\n","        x = self.encoder_lin(x) \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSgCVL1HYuVf"},"source":["# Decoder definition\n","\n","class Decoder(nn.Module):\n","    \n","    def __init__(self, encoded_space_dim):\n","        super().__init__()\n","\n","        ### Linear section\n","        self.decoder_lin = nn.Sequential(\n","            # First linear layer\n","            nn.Linear(encoded_space_dim, 64),\n","            nn.ReLU(True),\n","            # Second linear layer\n","            nn.Linear(64, 3 * 3 * 32),\n","            nn.ReLU(True)\n","        )\n","\n","        ### Unflatten\n","        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n","\n","        ### Convolutional section\n","        self.decoder_conv = nn.Sequential(\n","            # First transposed convolution\n","            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, output_padding=0),\n","            nn.ReLU(True),\n","            # Second transposed convolution\n","            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(True),\n","            # Third transposed convolution\n","            nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),\n","        )\n","        \n","    def forward(self, x):\n","        # Apply linear layers\n","        x = self.decoder_lin(x)\n","        # Unflatten\n","        x = self.unflatten(x)\n","        # Apply transposed convolutions\n","        x = self.decoder_conv(x)\n","        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n","        x = torch.sigmoid(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCyi0shuZiN1"},"source":["### Training and Testing"]},{"cell_type":"code","metadata":{"id":"SooNBq7-Zlhb"},"source":["### Training function\n","def train_epoch(encoder, decoder, device, dataloader, loss_func, optimizer):\n","    # Set train mode for both the encoder and the decoder\n","    encoder.train()\n","    decoder.train()\n","    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","    for image_batch, _ in dataloader:\n","        # Move tensor to the proper device\n","        image_batch = image_batch.to(device)\n","        # Encode data\n","        encoded_data = encoder(image_batch)\n","        # Decode data\n","        decoded_data = decoder(encoded_data)\n","        # Evaluate loss\n","        loss = loss_func(decoded_data, image_batch)\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        ## Print batch loss\n","        #print('\\t partial train loss (single batch): %f' % (loss.data))\n","    return loss.data   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMX-C2wbaLiJ"},"source":["### Testing function\n","def test_epoch(encoder, decoder, device, dataloader, loss_func):\n","    # Set evaluation mode for encoder and decoder\n","    encoder.eval()\n","    decoder.eval()\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        conc_out = []\n","        conc_label = []\n","        for image_batch, _ in dataloader:\n","            # Move tensor to the proper device\n","            image_batch = image_batch.to(device)\n","            # Encode data\n","            encoded_data = encoder(image_batch)\n","            # Decode data\n","            decoded_data = decoder(encoded_data)\n","            # Append the network output and the original image to the lists\n","            conc_out.append(decoded_data.cpu())\n","            conc_label.append(image_batch.cpu())\n","        # Create a single tensor with all the values in the lists\n","        conc_out = torch.cat(conc_out)\n","        conc_label = torch.cat(conc_label) \n","        # Evaluate global loss\n","        val_loss = loss_func(conc_out, conc_label)\n","    return val_loss.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_z-vSa4DaNji"},"source":["# Fit function\n","def fit(num_epochs, encoder, decoder, device, train_dataloader, loss_func, optim, test_dataloader, test_dataset):\n","    \n","    train_loss_log = []\n","    val_loss_log = [] \n","\n","    # Early stopping\n","    early_stopping = EarlyStopping(patience=5, verbose=False)\n","\n","    for epoch in range(num_epochs):\n","        print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n","\n","        ### Training (use the training function)\n","        train_loss = train_epoch(\n","            encoder=encoder, \n","            decoder=decoder, \n","            device=device, \n","            dataloader=train_dataloader, \n","            loss_func=loss_func, \n","            optimizer=optim)\n","        train_loss_log.append(train_loss)\n","        ### Validation (use the testing function)\n","\n","        val_loss = test_epoch(\n","            encoder=encoder, \n","            decoder=decoder, \n","            device=device, \n","            dataloader=test_dataloader, \n","            loss_func=loss_func)\n","        val_loss_log.append(val_loss)\n","\n","        # Print Validationloss\n","        print('\\n\\n\\t VALIDATION - EPOCH %d/%d - loss: %f\\n\\n' % (epoch + 1, num_epochs, val_loss))\n","\n","        # and if it has, it will make a checkpoint of the current model\n","        early_stopping(val_loss, decoder)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping\")\n","            break\n","\n","        # Save network parameters\n","        torch.save(encoder.state_dict(), 'encoder_params.pth')\n","        torch.save(decoder.state_dict(), 'decoder_params.pth')\n","        torch.save(optim.state_dict(), 'optim_params.pth')\n","\n","    return train_loss_log, val_loss_log "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFoAjXyGbhSp"},"source":["### Grid Search for model selection"]},{"cell_type":"code","metadata":{"id":"f0DX7melbg6I"},"source":["dict_params = {\n","            'LearningRate'    : [0.1, 0.01, 0.001, 0.0001],\n","            'Regularization'  : [1e-3, 1e-4, 1e-5, 1e-6],\n","            'Epochs'          : [1000],\n","            'EncodedSpaceDim' : [4, 8, 16]\n","         }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrutX4f4bBsY"},"source":["comb_params = list(itertools.product(*dict_params.values()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Iihd-EroNYy"},"source":["par_log = []\n","train_loss_log = []\n","val_loss_log = []\n","\n","for iter, params in enumerate(comb_params):\n","    print('Iteration:', iter)\n","\n","    par_log.append(params)\n","\n","    loss_func = nn.MSELoss()\n","    lr, lamb, num_epochs,  encoded_space_dim = params\n","\n","    encoder = Encoder(encoded_space_dim = encoded_space_dim) \n","    decoder = Decoder(encoded_space_dim = encoded_space_dim)\n","\n","    autoenc_params = [{'params': encoder.parameters()}, {'params': decoder.parameters()}]\n","    optim = torch.optim.Adam(autoenc_params, lr=lr, weight_decay=lamb) \n","\n","    encoder.to(device)\n","    decoder.to(device)\n","\n","    # Training\n","    train_loss, val_loss = fit(num_epochs = num_epochs,\n","                                       encoder = encoder,\n","                                       decoder = decoder,\n","                                       device = device,\n","                                       train_dataloader = train_dataloader,\n","                                       loss_func = loss_func,\n","                                       optim = optim,\n","                                       test_dataloader = test_dataloader,\n","                                       test_dataset = test_dataset)\n","    \n","    train_loss_log.append(train_loss[-1])\n","    val_loss_log.append(val_loss[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4uxIshYulQm"},"source":["# Select best parameters\n","best_params = par_log[np.argmin([v[-1] for v in val_loss_log])]\n","best_params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Cy1fsoBu3SC"},"source":["### Train with best parameters"]},{"cell_type":"code","metadata":{"id":"NfJKCXu_u30_"},"source":["best_params = (0.001, 1e-5, 1000, 16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BV57PU4Mu_pe"},"source":["loss_func = nn.MSELoss()\n","lr, lamb, num_epochs,  encoded_space_dim = best_params\n","\n","encoder = Encoder(encoded_space_dim = encoded_space_dim) \n","decoder = Decoder(encoded_space_dim = encoded_space_dim)\n","\n","autoenc_params = [{'params': encoder.parameters()}, {'params': decoder.parameters()}]\n","optim = torch.optim.Adam(autoenc_params, lr=lr, weight_decay=lamb) \n","\n","encoder.to(device)\n","decoder.to(device)\n","\n","# Training\n","train_loss, val_loss = fit(num_epochs = num_epochs,\n","                                       encoder = encoder,\n","                                       decoder = decoder,\n","                                       device = device,\n","                                       train_dataloader = train_dataloader,\n","                                       loss_func = loss_func,\n","                                       optim = optim,\n","                                       test_dataloader = test_dataloader,\n","                                       test_dataset = test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BxzDsrKvdfe"},"source":["print('Training loss:', train_loss[-1].detach().cpu().numpy())\n","print('Validation loss:', val_loss[-1].detach().cpu().numpy())\n","\n","# Save network parameters\n","torch.save(encoder.state_dict(), 'encoder_params.pth')\n","torch.save(decoder.state_dict(), 'decoder_params.pth')\n","torch.save(optim.state_dict(), 'optim_params.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_6axEr6voIk"},"source":["# Plot Training and Validation loss\n","plt.plot(train_loss, label='Training')\n","plt.plot(val_loss, label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.savefig('TrainValLoss_AE.pdf', bbox_inches='tight')\n","files.download('TrainValLoss_AE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTbheqtY1huh"},"source":["## Test the trained model"]},{"cell_type":"code","metadata":{"id":"lfa7k_F31tH3"},"source":["test_loss = test_epoch(encoder = encoder,\n","                       decoder = decoder,\n","                       device = device,\n","                       dataloader = test_dataloader,\n","                       loss_func = loss_func)\n","\n","print('Test loss', test_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"db1DTE4OTZiz"},"source":["# Example of reconstructed images\n","img1 = test_dataset[0][0].unsqueeze(0).to(device)\n","img2 = test_dataset[1][0].unsqueeze(0).to(device)\n","img3 = test_dataset[2][0].unsqueeze(0).to(device)\n","encoder.eval()\n","decoder.eval()\n","with torch.no_grad():\n","    rec_img1 = decoder(encoder(img1))\n","    rec_img2 = decoder(encoder(img2))\n","    rec_img3 = decoder(encoder(img3))\n","# Plot the reconstructed image\n","fig, axs = plt.subplots(2, 2, figsize=(10,11))\n","# Image 1\n","axs[0,0].imshow(img1.cpu().squeeze().numpy(), cmap='gist_gray')\n","axs[0,0].set_title('Original image')\n","axs[0,1].imshow(rec_img1.cpu().squeeze().numpy(), cmap='gist_gray')\n","axs[0,1].set_title('Reconstructed image')\n","# Image 2\n","axs[1,0].imshow(img2.cpu().squeeze().numpy(), cmap='gist_gray')\n","axs[1,0].set_title('Original image')\n","axs[1,1].imshow(rec_img2.cpu().squeeze().numpy(), cmap='gist_gray')\n","axs[1,1].set_title('Reconstructed image')\n","\n","# Save figures\n","fig.savefig('Reconstruct_AE.pdf', bbox_inches='tight')\n","files.download('Reconstruct_AE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4U_PUDqATwFL"},"source":["## Denoiser"]},{"cell_type":"code","metadata":{"id":"uUdSYpYyTiHi"},"source":["# Introduce noise on imges in Training and Testing functions\n","\n","# Training function\n","def train_epoch(encoder, decoder, device, dataloader, loss_func, optimizer):\n","    # Set train mode for both the encoder and the decoder\n","    encoder.train()\n","    decoder.train()\n","    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n","        # Move tensor to the proper device\n","        image_batch = image_batch.to(device)\n","        # Add noise \n","        mean = torch.randn(1).to(device)\n","        std = torch.randn(1).to(device) * 0.5 + 0.5\n","        noisy_image = image_batch + torch.randn(image_batch.size()).to(device) * std + mean\n","        # Encode data\n","        encoded_data = encoder(image_batch)\n","        # Decode data\n","        decoded_data = decoder(encoded_data)\n","        # Evaluate loss\n","        loss = loss_func(decoded_data, image_batch)\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        ## Print batch loss\n","        #print('\\t partial train loss (single batch): %f' % (loss.data))\n","    return loss.data  \n","\n","# Testing function\n","def test_epoch(encoder, decoder, device, dataloader, loss_func):\n","    # Set evaluation mode for encoder and decoder\n","    encoder.eval()\n","    decoder.eval()\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        conc_out = []\n","        conc_label = []\n","        for image_batch, _ in dataloader:\n","            # Move tensor to the proper device\n","            image_batch = image_batch.to(device)\n","            # Add noise\n","            mean = torch.randn(1).to(device)\n","            std = torch.randn(1).to(device) * 0.5 + 0.5\n","            noisy_image = image_batch + torch.randn(image_batch.size()).to(device) * std + mean \n","            # Encode data\n","            encoded_data = encoder(image_batch)\n","            # Decode data\n","            decoded_data = decoder(encoded_data)\n","            # Append the network output and the original image to the lists\n","            conc_out.append(decoded_data.cpu())\n","            conc_label.append(image_batch.cpu())\n","        # Create a single tensor with all the values in the lists\n","        conc_out = torch.cat(conc_out)\n","        conc_label = torch.cat(conc_label) \n","        # Evaluate global loss\n","        val_loss = loss_func(conc_out, conc_label)\n","    return val_loss.data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJ_IINAYUh6F"},"source":["### Train with best parameters"]},{"cell_type":"code","metadata":{"id":"PiYzmJC7Uh6G"},"source":["best_params = (0.001, 1e-5, 150, 16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d60v__cKUh6H"},"source":["loss_func = nn.MSELoss()\n","lr, lamb, num_epochs,  encoded_space_dim = best_params\n","\n","encoder = Encoder(encoded_space_dim = encoded_space_dim) \n","decoder = Decoder(encoded_space_dim = encoded_space_dim)\n","\n","autoenc_params = [{'params': encoder.parameters()}, {'params': decoder.parameters()}]\n","optim = torch.optim.Adam(autoenc_params, lr=lr, weight_decay=lamb) \n","\n","encoder.to(device)\n","decoder.to(device)\n","\n","# Training\n","train_loss, val_loss = fit(num_epochs = num_epochs,\n","                                       encoder = encoder,\n","                                       decoder = decoder,\n","                                       device = device,\n","                                       train_dataloader = train_dataloader,\n","                                       loss_func = loss_func,\n","                                       optim = optim,\n","                                       test_dataloader = test_dataloader,\n","                                       test_dataset = test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N18_YT6iUh6K"},"source":["print('Training loss:', train_loss[-1].detach().cpu().numpy())\n","print('Validation loss:', val_loss[-1].detach().cpu().numpy())\n","\n","# Save network parameters\n","torch.save(encoder.state_dict(), 'encoder_params.pth')\n","torch.save(decoder.state_dict(), 'decoder_params.pth')\n","torch.save(optim.state_dict(), 'optim_params.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiJ08VgSUh6L"},"source":["# Plot Training and Validation loss\n","plt.plot(train_loss, label='Training')\n","plt.plot(val_loss, label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.savefig('TrainValLoss_DAE.pdf', bbox_inches='tight')\n","files.download('TrainValLoss_DAE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1vybO6WUh6M"},"source":["## Test the trained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDT_--duUh6N","executionInfo":{"status":"ok","timestamp":1629914543745,"user_tz":-120,"elapsed":1703,"user":{"displayName":"Michele Puppin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRS_KXhTLE8H-EFQyu3ENF4tW0st3lMob2BMsA=s64","userId":"10648899063932926796"}},"outputId":"b49287af-27f8-460b-9888-5e62de89938a"},"source":["test_loss = test_epoch(encoder = encoder,\n","                       decoder = decoder,\n","                       device = device,\n","                       dataloader = test_dataloader,\n","                       loss_func = loss_func)\n","\n","print('Test loss', test_loss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test loss tensor(0.0094)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jvqfP_9YUjAA"},"source":["fig, axs = plt.subplots(3, 3, figsize=(12,12))\n","for ax in axs:\n","    # Original\n","    img, label = random.choice(test_dataset)\n","    imgc = img[0]\n","    ax[0].imshow(np.array(imgc), cmap='gist_gray')\n","    ax[0].set_xticks([])\n","    ax[0].set_yticks([])\n","    ax[0].set_title('Original')\n","    # Noisy\n","    imgc += np.random.normal(0, 1, size=imgc.shape)\n","    ax[1].imshow(np.array(imgc), cmap='gist_gray')\n","    ax[1].set_xticks([])\n","    ax[1].set_yticks([])\n","    ax[1].set_title('Noisy')\n","    # Denoised\n","    encoder.eval()\n","    decoder.eval()\n","    img = img.unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        enc_img = encoder(img)\n","        dec_img = decoder(enc_img)\n","    ax[2].imshow(np.array(dec_img.detach().cpu()[0][0]), cmap='gist_gray')\n","    ax[2].set_xticks([])\n","    ax[2].set_yticks([])\n","    ax[2].set_title('Decoded')\n","\n","plt.savefig('Reconstruct_DAE.pdf', bbox_inches='tight')\n","files.download('Reconstruct_DAE.pdf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pfL4hVVSwNaG"},"source":["## Fine Tuning with Supervised Classification"]},{"cell_type":"code","metadata":{"id":"WYDfrcWqwXgk"},"source":["# Initialize the decoder and load trained model \n","encoder = Encoder(encoded_space_dim = 16)\n","encoder.load_state_dict(torch.load('encoder_params.pth'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrmmNZJG0QqS"},"source":["class Classification(nn.Module):\n","\n","    def __init__(self, encoded_space_dim, pretrained_encoder):\n","        super().__init__()\n","        # Encoder\n","        self.encoder = pretrained_encoder\n","        # Linear readout for classification\n","        self.lin_readout = nn.Sequential(nn.Linear(encoded_space_dim, 64),\n","                                             nn.ReLU(True),\n","                                             nn.Linear(64, 10),\n","                                             nn.LogSoftmax(dim=-1))\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.lin_readout(x)\n","        return x\n","\n","    def train_nn(self, train_loader, optimizer, loss_func, device):\n","        train_loss= []\n","        self.train()\n","        for sample_batched in train_loader:\n","            x_batch = sample_batched[0].to(device)\n","            label_batch = sample_batched[1].to(device)\n","            out = self.forward(x_batch)\n","            loss = loss_func(out, label_batch)\n","            self.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_batch = loss.detach().cpu().numpy()\n","            train_loss.append(loss_batch)\n","        return train_loss\n","    \n","    def validation_nn(self, val_loader, loss_func, device):\n","        val_loss = []\n","        self.eval() \n","        with torch.no_grad():\n","            for sample_batched in val_loader:\n","                x_batch = sample_batched[0].to(device)\n","                label_batch = sample_batched[1].to(device)\n","                out = self.forward(x_batch)\n","                loss = loss_func(out, label_batch)\n","                loss_batch = loss.detach().cpu().numpy()\n","                val_loss.append(loss_batch)\n","        return val_loss\n","    \n","    def fit(self, train_loader, val_loader, optimizer, loss_func, epochs, device):\n","        train_loss_log = []\n","        val_loss_log = []\n","\n","        early_stopping = EarlyStopping(patience = 5, verbose = False)\n","\n","        for epoch in range(epochs):\n","            print(epoch)\n","            # Training\n","            train_loss = self.train_nn(train_loader, optimizer, loss_func, device)\n","            train_loss_log.append(np.mean(train_loss))\n","            # Validation\n","            val_loss = self.validation_nn(val_loader, loss_func, device)\n","            val_loss_log.append(np.mean(val_loss))\n","\n","            early_stopping(np.mean(val_loss), self)\n","            if early_stopping.early_stop:\n","                print(\"Early stopping\")\n","                break\n","            \n","        return train_loss_log, val_loss_log\n","\n","    def predict(self, input_loader, loss_func, device):\n","        inputs = []\n","        outputs = []\n","        labels = []\n","        self.eval()\n","        with torch.no_grad(): \n","            for sample_batched in input_loader:\n","                x_batch = sample_batched[0].to(device)\n","                label = sample_batched[1].to(device) \n","                out = self.forward(x_batch)\n","                inputs.append(x_batch)\n","                outputs.append(out)\n","                labels.append(label) \n","        inputs = torch.cat(inputs)\n","        outputs = torch.cat(outputs)\n","        labels = torch.cat(labels)\n","        test_loss = loss_func(outputs, labels) \n","        return inputs, outputs, labels, test_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lXa01qj4FDj2"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"m6rk75AN0Qt4"},"source":["model_class = Classification(encoded_space_dim=16, pretrained_encoder=encoder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgYzRSPb0QwE"},"source":["# Fix encoder weights\n","for param_name, param in model_class.encoder.named_parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoxBf5uw0QyH"},"source":["loss_func = nn.NLLLoss()\n","optim = torch.optim.Adam(model_class.parameters(), lr=0.001, weight_decay=1e-5) \n","\n","model_class.to(device) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Dsc7Zk20dlF"},"source":["# Prepare dataset\n","train_set, val_set = torch.utils.data.random_split(train_dataset, [int(0.2*len(train_dataset)), int(0.8*len(train_dataset))])\n","\n","train_dataloader = DataLoader(train_set, batch_size=256, shuffle=True)\n","val_dataloader = DataLoader(val_set, batch_size=256, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQul4LuX0doY"},"source":["train_loss, val_loss = model_class.fit(train_loader = train_dataloader, \n","                                             val_loader = val_dataloader, \n","                                             loss_func = loss_func, \n","                                             optimizer = optim, \n","                                             epochs = 20, \n","                                             device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZR4OCvT0dqo"},"source":["# Plot Training and Validation loss\n","plt.plot(train_loss, label='Training')\n","plt.plot(val_loss, label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.savefig('TrainValLoss_CLassAE.pdf', bbox_inches='tight')\n","files.download('TrainValLoss_CLassAE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DX1VA8zFI-u"},"source":["### Test the model"]},{"cell_type":"code","metadata":{"id":"le-Ghdrk1Fua"},"source":["test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n","\n","inputs, outputs, labels, test_loss = model_class.predict(test_dataloader, loss_func, device)\n","\n","print(\"Test loss:\", test_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20OKGjNx1F1C"},"source":["# Compute test accuracy\n","outputs = outputs.detach().cpu().numpy()\n","labels = labels.detach().cpu().numpy()\n","\n","predicted_labels = [outputs[i].argmax() for i in range(len(outputs))]\n","diffs = np.array([predicted_labels[i]-labels[i] for i in range(len(outputs))])\n","wrong = np.count_nonzero(diffs) \n","test_accuracy = 1 - wrong/len(outputs)\n","print(\"Test accuracy: \", test_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBxYSaDcIQj8"},"source":["### Confusion matrix for the test set"]},{"cell_type":"code","metadata":{"id":"8InMy-wKISpi"},"source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues,\n","\t\t\t\t\t\t  save_path='models/'):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        #print(\"Normalized confusion matrix\")\n","    #else:\n","        #print('Confusion matrix, without normalization')\n","\n","    plt.figure(figsize=(15, 15))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=30)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45, fontsize=15)\n","    plt.yticks(tick_marks, classes, fontsize=15)\n","\n","    fmt = '.3f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), size=11,\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label', fontsize=30)\n","    plt.xlabel('Predicted label', fontsize=30)\n","    plt.savefig(save_path+\"_picConfMatrix.png\", dpi=400)\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDgF3Ar3ISsE"},"source":["# Confusion Matrix\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(labels, predicted_labels)\n","categories=[0,1,2,3,4,5,6,7,8,9]\n","plot_confusion_matrix(cm,categories, normalize=False,save_path='./confusion.pdf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFP-aLO7Ic-S"},"source":["# Explore the latent space structure"]},{"cell_type":"code","metadata":{"id":"H9-qnX1WI99K"},"source":["from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.cluster import DBSCAN \n","import plotly.express as px"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHBuZHblItOx","executionInfo":{"status":"ok","timestamp":1629920812992,"user_tz":-120,"elapsed":9,"user":{"displayName":"Michele Puppin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRS_KXhTLE8H-EFQyu3ENF4tW0st3lMob2BMsA=s64","userId":"10648899063932926796"}},"outputId":"55731509-85c9-4216-b991-613fdc132c48"},"source":["# Load network parameters\n","encoder.load_state_dict(torch.load('encoder_params.pth'))\n","decoder.load_state_dict(torch.load('decoder_params.pth'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"v1HA5nfiIvEb"},"source":["# Get the encoded representation of the test samples\n","encoded_samples = []\n","for sample in tqdm(test_dataset):\n","    img = sample[0].unsqueeze(0).to(device)\n","    label = sample[1]\n","    # Encode image\n","    encoder.eval()\n","    with torch.no_grad():\n","        encoded_img  = encoder(img)\n","    # Append to list\n","    encoded_img = encoded_img.flatten().cpu().numpy()\n","    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n","    encoded_sample['label'] = label\n","    encoded_samples.append(encoded_sample)\n","\n","# Convert to a dataframe\n","encoded_samples = pd.DataFrame(encoded_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1tARPJVIvgD"},"source":["px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffh9d0urIvig"},"source":["n_components = 2\n","pca = PCA(n_components=n_components)\n","pca.fit(encoded_samples.iloc[:, 0:encoded_space_dim])\n","\n","pca_transf_samples = pd.DataFrame(pca.transform(encoded_samples.iloc[:, 0:encoded_space_dim]), \n","                                  columns=[f'Enc. Variable {i}' for i in range(n_components)])\n","\n","fig = px.scatter(pca_transf_samples, \n","                 x='Enc. Variable 0', \n","                 y='Enc. Variable 1', \n","                 color=encoded_samples.label.astype(str), \n","                 opacity=0.7)\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"djmQqTygIvla"},"source":["n_components = 2\n","tsne = TSNE(n_components=n_components)\n","tsne.fit(encoded_samples.iloc[:, 0:encoded_space_dim])\n","\n","tsne_tranf_samples = pd.DataFrame(tsne.fit_transform(encoded_samples.iloc[:, 0:encoded_space_dim]), \n","                                  columns=[f'Enc. Variable {i}' for i in range(n_components)])\n","\n","fig = px.scatter(tsne_tranf_samples, \n","                 x='Enc. Variable 0', \n","                 y='Enc. Variable 1', \n","                 color=encoded_samples.label.astype(str), \n","                 opacity=0.7)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zd5JUnAsJH4E"},"source":["# Generate new samples"]},{"cell_type":"code","metadata":{"id":"XGu674VJJJ-a"},"source":["# Generate a custom sample\n","custom_encoded_sample1 = np.random.randn(16)*50\n","encoded_value1 = torch.tensor(custom_encoded_sample1).float().unsqueeze(0).to(device)\n","custom_encoded_sample2 = np.random.randn(16)*50\n","encoded_value2 = torch.tensor(custom_encoded_sample2).float().unsqueeze(0).to(device)\n","custom_encoded_sample3 = np.random.randn(16)*50\n","encoded_value3 = torch.tensor(custom_encoded_sample3).float().unsqueeze(0).to(device)\n","\n","# Decode sample\n","decoder.eval()\n","with torch.no_grad():\n","    generated_img1 = decoder(encoded_value1)\n","    generated_img2 = decoder(encoded_value2)\n","    generated_img3 = decoder(encoded_value3)\n","\n","# Plot the reconstructed image\n","fig, axs = plt.subplots(1, 3, figsize=(12,6))\n","axs[0].imshow(generated_img1.cpu().squeeze().numpy(), cmap='gist_gray')\n","axs[1].imshow(generated_img2.cpu().squeeze().numpy(), cmap='gist_gray')\n","axs[2].imshow(generated_img3.cpu().squeeze().numpy(), cmap='gist_gray')\n","plt.tight_layout()\n","# Save figures\n","fig.savefig('GenSamp_AE.pdf', bbox_inches='tight')\n","files.download('GenSamp_AE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rgki4sUxKHon"},"source":["# Variational Autoencoders"]},{"cell_type":"code","metadata":{"id":"IkWybB8AKJvI"},"source":["# Define Variational AutoEncoder\n","class VAE(nn.Module):\n","    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n","        super(VAE, self).__init__()\n","        \n","        # encoder part\n","        self.fc1 = nn.Linear(x_dim, h_dim1)\n","        self.fc2 = nn.Linear(h_dim1, h_dim2)\n","        self.fc31 = nn.Linear(h_dim2, z_dim)\n","        self.fc32 = nn.Linear(h_dim2, z_dim)\n","        # decoder part\n","        self.fc4 = nn.Linear(z_dim, h_dim2)\n","        self.fc5 = nn.Linear(h_dim2, h_dim1)\n","        self.fc6 = nn.Linear(h_dim1, x_dim)\n","\n","        self.act1 = nn.ReLU()\n","        self.act2 = nn.Sigmoid()\n","\n","        \n","    def encoder(self, x):\n","        h = self.act1(self.fc1(x))\n","        h = self.act1(self.fc2(h))\n","        return self.fc31(h), self.fc32(h) # mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","    def decoder(self, z):\n","        h = self.act1(self.fc4(z))\n","        h = self.act1(self.fc5(h))\n","        return self.act2(self.fc6(h)) \n","    \n","    def forward(self, x):\n","        mu, log_var = self.encoder(x.view(-1, 784))\n","        z = self.sampling(mu, log_var)\n","        return self.decoder(z), mu, log_var\n","\n","def train():\n","    vae.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_dataloader):\n","        data = data[0].cuda()\n","        optimizer.zero_grad()\n","        \n","        recon_batch, mu, log_var = vae(data)\n","        loss = loss_function(recon_batch, data, mu, log_var)\n","        \n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","\n","    return train_loss / len(train_dataloader.dataset)\n","\n","\n","\n","def test():\n","    vae.eval()\n","    test_loss= 0\n","    with torch.no_grad():\n","        for data, _ in test_dataloader:\n","            data = data[0].cuda()\n","            recon, mu, log_var = vae(data)\n","            \n","            # sum up batch loss\n","            test_loss += loss_function(recon, data, mu, log_var).item()\n","        \n","    test_loss /= len(test_dataloader.dataset)\n","\n","    return test_loss "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3rDJ1u5TOZO"},"source":["vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIroIWpPKKVK"},"source":["vae.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpKaVSfSKKXu"},"source":["optimizer = torch.optim.Adam(vae.parameters())\n","\n","def loss_function(recon_x, x, mu, log_var):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","    return BCE + KLD"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-7x-y4IKKfM"},"source":["train_loss = []\n","test_loss = []\n","\n","for epoch in range(1, 51):\n","    print('Epoch: ', epoch)\n","    train_loss.append(train())\n","    test_loss.append(test())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBmu28MfPQWu"},"source":["# Plot Training and Validation loss\n","plt.plot(train_loss, label='Training')\n","plt.plot(test_loss, label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.savefig('TrainValLoss_VAE.pdf', bbox_inches='tight')\n","files.download('TrainValLoss_VAE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFWDkV8IaDyi"},"source":["# Generate a custom sample\n","encoded_value1 = torch.randn(1, 9).to(device)\n","encoded_value2 = torch.randn(1, 9).to(device)\n","encoded_value3 = torch.randn(1, 9).to(device)\n","\n","\n","# Decode sample\n","decoder.eval()\n","with torch.no_grad():\n","    generated_img1 = vae.decoder(encoded_value1).to(device)\n","    generated_img2 = vae.decoder(encoded_value2).to(device)\n","    generated_img3 = vae.decoder(encoded_value3).to(device)\n","\n","generated_img1 = generated_img1.view(1, 1, 28, 28)[0].cpu().squeeze().numpy()\n","generated_img2 = generated_img2.view(1, 1, 28, 28)[0].cpu().squeeze().numpy()\n","generated_img3 = generated_img3.view(1, 1, 28, 28)[0].cpu().squeeze().numpy()\n","\n","# Plot the reconstructed image\n","fig, axs = plt.subplots(1, 3, figsize=(12,6))\n","axs[0].imshow(generated_img1, cmap='gist_gray')\n","axs[1].imshow(generated_img2, cmap='gist_gray')\n","axs[2].imshow(generated_img3, cmap='gist_gray')\n","plt.tight_layout()\n","# Save figures\n","\n","plt.savefig('GenSamp_VAE.pdf', bbox_inches='tight')\n","files.download('GenSamp_VAE.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]}]}