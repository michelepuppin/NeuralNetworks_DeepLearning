{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Puppin_Homework_1-Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TJS0NPOU41Pq"},"source":["#NEURAL NETWORKS AND DEEP LEARNING\n","\n","## Homework 1 - Supervised Deep Learning\n","\n","### Classification task\n","\n","Puppin Michele - 1227474"]},{"cell_type":"code","metadata":{"id":"9VP6DQBv45SG"},"source":["# Import packages\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","import torch.nn.functional as F\n","\n","# Set the seed\n","np.random.seed(25)\n","torch.manual_seed(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F0zCY3BX49I0"},"source":["# Set device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Training device: {device}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vnVtWHNP5PjJ"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"1llqMITv49Mo"},"source":["train_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True)\n","test_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAvlgPF_5jb_"},"source":["## Early stopping"]},{"cell_type":"code","metadata":{"id":"YbZxxr7449OQ"},"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Va0Hw55e5rc8"},"source":["## Network definition"]},{"cell_type":"code","metadata":{"id":"OmopN2rK49Q5"},"source":["class Net(nn.Module):\n","\n","    def __init__(self, DropProb = 0):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1,  out_channels=16, kernel_size=3, stride=1, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.drop = nn.Dropout(DropProb)\n","        self.fc1 = nn.Linear(32*8*8, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","        self.act = nn.ReLU()\n","        print('Network initialized')\n","\n","    def forward(self, x, return_layer = 0):\n","        x = self.act(self.conv1(x))\n","        if return_layer == 1:\n","          return x\n","        x = self.pool(x)\n","        x = self.act(self.conv2(x))\n","        if return_layer == 2:\n","          return x\n","        x = self.pool(x)\n","        x = torch.flatten(x, 1) \n","        x = self.drop(x)\n","        x = self.act(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","    \n","    def train_nn(self, train_loader, optimizer, loss_func, device):\n","        train_loss= []\n","        self.train()\n","        for sample_batched in train_loader:\n","            x_batch = sample_batched[0].to(device)\n","            label_batch = sample_batched[1].to(device)\n","            out = self.forward(x_batch)\n","            loss = loss_func(out, label_batch)\n","            self.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_batch = loss.detach().cpu().numpy()\n","            train_loss.append(loss_batch)\n","        return train_loss\n","    \n","    def validation_nn(self, val_loader, loss_func, device):\n","        val_loss = []\n","        self.eval() \n","        with torch.no_grad():\n","            for sample_batched in val_loader:\n","                x_batch = sample_batched[0].to(device)\n","                label_batch = sample_batched[1].to(device)\n","                out = self.forward(x_batch)\n","                loss = loss_func(out, label_batch)\n","                loss_batch = loss.detach().cpu().numpy()\n","                val_loss.append(loss_batch)\n","        return val_loss\n","    \n","    def fit(self, train_loader, val_loader, optimizer, loss_func, epochs, device):\n","        train_loss_log = []\n","        val_loss_log = []\n","\n","        early_stopping = EarlyStopping(patience = 25, verbose = False)\n","\n","        for epoch in range(epochs):\n","            print(epoch)\n","            # Training\n","            train_loss = self.train_nn(train_loader, optimizer, loss_func, device)\n","            train_loss_log.append(np.mean(train_loss))\n","            # Validation\n","            val_loss = self.validation_nn(val_loader, loss_func, device)\n","            val_loss_log.append(np.mean(val_loss))\n","\n","            early_stopping(np.mean(val_loss), self)\n","            if early_stopping.early_stop:\n","                print(\"Early stopping\")\n","                break\n","            \n","        return train_loss_log, val_loss_log\n","\n","    def predict(self, input_loader, loss_func, device):\n","        inputs = []\n","        outputs = []\n","        labels = []\n","        self.eval()\n","        with torch.no_grad(): \n","            for sample_batched in input_loader:\n","                x_batch = sample_batched[0].to(device)\n","                label = sample_batched[1].to(device) \n","                out = self.forward(x_batch)\n","                inputs.append(x_batch)\n","                outputs.append(out)\n","                labels.append(label) \n","        inputs = torch.cat(inputs)\n","        outputs = torch.cat(outputs)\n","        labels = torch.cat(labels)\n","        test_loss = loss_func(outputs, labels) \n","        return inputs, outputs, labels, test_loss\n","\n","    def save(self, path):\n","        torch.save(self.state_dict(), path)\n","        \n","    def load(self, path):\n","        self.load_state_dict( torch.load(path) )\n","        \n","    def restart(self):\n","        self.__init__()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jewJf02x6Krb"},"source":["## Random Grid Search"]},{"cell_type":"code","metadata":{"id":"QaoY758j6PyH"},"source":["def RandomGridSearch(config, train_load, val_load, rep, device):\n","\n","    par_log = []\n","    train_loss_log = []\n","    val_loss_log = []\n","\n","    for i in range(rep):\n","        print(i)\n","        \n","        sample_params = {}\n","\n","        for k in config.keys():\n","            sample_params[k] = np.random.choice(config[k])\n","\n","        par_log.append(sample_params)\n","\n","        DropProb = sample_params['Dropout']\n","\n","        model = Net(DropProb).to(device)\n","\n","        loss_func = nn.CrossEntropyLoss()\n","        epochs = sample_params['Epochs']\n","\n","        if sample_params['Optimizer']=='Adam':\n","                opt = optim.Adam(model.parameters(), lr = sample_params['LearningRate'], weight_decay = sample_params['Regularization'])\n","        if sample_params['Optimizer']=='SGD':\n","                opt = optim.SGD( model.parameters(), lr = sample_params['LearningRate'], weight_decay = sample_params['Regularization'], momentum=0.9)\n","\n","        # Training & validation\n","        train_loss, val_loss = model.fit(train_load, val_load, opt, loss_func, epochs, device)\n","            \n","        # Storing train/loss validation\n","        train_loss_log.append( train_loss )\n","        val_loss_log.append( val_loss )\n","\n","    return par_log, train_loss_log, val_loss_log "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4Alc3Na8aKA"},"source":["## Training and Testing the network"]},{"cell_type":"markdown","metadata":{"id":"rwHcVd7X8aMD"},"source":["### Model selection"]},{"cell_type":"code","metadata":{"id":"lPJiptuO6P3o"},"source":["dict_params = {\n","            'LearningRate'    : [0.01, 0.001, 0.0001],\n","            'Regularization'  : [1e-4, 1e-5, 1e-6],\n","            'Dropout'         : [0, 0.1, 0.15],\n","            'Epochs'          : [20, 40, 60],\n","            'Optimizer'       : ['SGD', 'Adam']\n","         }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2i5mZMsj49Tw"},"source":["# Add random transformation and normalization\n","add_noise  = torchvision.transforms.GaussianBlur(kernel_size=3)\n","rotation   = torchvision.transforms.RandomRotation(30)\n","distortion = torchvision.transforms.RandomPerspective(0.5)\n","\n","random_transform = transforms.RandomChoice([add_noise, rotation, distortion])\n","composed_transform = transforms.Compose( [ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAQl-IfV-l43"},"source":["train_dataset = torchvision.datasets.MNIST('classifier_data', \n","                                           transform=composed_transform, \n","                                           train=True, \n","                                           download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVllHhSG6P0Y"},"source":["train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n","train_load = DataLoader(train_set, batch_size=64, shuffle=True)\n","val_load = DataLoader(val_set, batch_size=64, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Q9K7WGd6P57"},"source":["params_list, train_loss_list, val_loss_list = RandomGridSearch(dict_params, train_load, val_load, 20, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIK7U50J_CsJ"},"source":["# Select best parameters\n","best_params = params_list[np.argmin([v[-1] for v in val_loss_list])]\n","best_params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRz-kyii-ztj"},"source":["### Train with best parameters"]},{"cell_type":"code","metadata":{"id":"4U91SJD5_dLn"},"source":["best_params = {\n","            'LearningRate'    : 0.0001,\n","            'Regularization'  : 1e-4,\n","            'Dropout'         : 0.15,\n","            'Epochs'          : 60,\n","            'Optimizer'       : 'Adam'\n","         }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MPb99TY-0A7"},"source":["train_load = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0 )\n","val_load = train_load\n","\n","DropProb = best_params['Dropout']\n","\n","model = Net(DropProb).to(device)\n","\n","loss_func = nn.CrossEntropyLoss()\n","epochs = best_params['Epochs']\n","\n","if best_params['Optimizer']=='Adam':\n","        opt = optim.Adam(model.parameters(), lr = best_params['LearningRate'], weight_decay = best_params['Regularization'])\n","if best_params['Optimizer']=='SGD':\n","        opt = optim.SGD( model.parameters(), lr = best_params['LearningRate'], weight_decay = best_params['Regularization'], momentum=0.9)\n","\n","# Training & validation\n","train_loss, val_loss = model.fit(train_load, val_load, opt, loss_func, epochs, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGb1O-1K87ww"},"source":["# Plot Training and Validation loss\n","plt.plot(train_loss, label='Training')\n","plt.plot(val_loss, label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.savefig('TrainValLoss_Class.pdf', bbox_inches='tight')\n","files.download('TrainValLoss_Class.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Oc4nc2V_r1z"},"source":["# Save trained model\n","model.save('net_class_parameters.torch')\n","torch.save(opt.state_dict(), 'optimizer_class_state.torch')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMZUm0Mf_6Tj"},"source":["### Test the trained model"]},{"cell_type":"code","metadata":{"id":"Qi7sT7TK_ySX"},"source":["# Load test set\n","test_dataset = torchvision.datasets.MNIST('classifier_data', \n","                                          transform=transforms.Compose([\n","                                                transforms.ToTensor(),\n","                                                transforms.Normalize((0.1307,), (0.3081,))]),\n","                                          train=False, \n","                                          download=True)\n","\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YxbU80gABrl"},"source":["# Run predictions\n","loss_func = nn.CrossEntropyLoss()\n","\n","model.load('net_class_parameters.torch')\n","\n","inputs, outputs, labels, test_loss = model.predict(test_loader, loss_func, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdHbmiC6AQQW"},"source":["# Evaluate test accuracy\n","outputs = outputs.detach().cpu().numpy()\n","labels = labels.detach().cpu().numpy()\n","\n","predicted_labels = [outputs[i].argmax() for i in range(len(outputs))]\n","diffs = np.array([predicted_labels[i]-labels[i] for i in range(len(outputs))])\n","wrong = np.count_nonzero(diffs) \n","test_accuracy = 1 - wrong/len(outputs)\n","print(\"Test accuracy: \", test_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNHwjKcjOgSS"},"source":["### Confusion matrix for the test set"]},{"cell_type":"code","metadata":{"id":"SUvtExu7AQVS"},"source":["import itertools\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues,\n","\t\t\t\t\t\t  save_path='models/'):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        #print(\"Normalized confusion matrix\")\n","    #else:\n","        #print('Confusion matrix, without normalization')\n","\n","    plt.figure(figsize=(15, 15))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=30)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45, fontsize=15)\n","    plt.yticks(tick_marks, classes, fontsize=15)\n","\n","    fmt = '.3f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), size=11,\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label', fontsize=30)\n","    plt.xlabel('Predicted label', fontsize=30)\n","    plt.savefig(save_path+\"_picConfMatrix.png\", dpi=400)\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gVL08ePAQZJ"},"source":["# Confusion Matrix\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(labels, predicted_labels)\n","categories=[0,1,2,3,4,5,6,7,8,9]\n","plot_confusion_matrix(cm,categories, normalize=False,save_path='./confusion.pdf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u4JVMoTjOnm1"},"source":["## Weights Histogram"]},{"cell_type":"code","metadata":{"id":"kN3_4rb9Onz2"},"source":["# First convolutional layer\n","c1_w = model.conv1.weight.data.cpu().numpy() \n","c1_b = model.conv1.bias.data.cpu().numpy() \n","\n","# Second convolutional layer\n","c2_w = model.conv2.weight.data.cpu().numpy()\n","c2_b = model.conv2.bias.data.cpu().numpy() \n","\n","# First hidden layer\n","h1_w = model.fc1.weight.data.cpu().numpy() \n","h1_b = model.fc1.bias.data.cpu().numpy() \n","\n","# Second hidden layer\n","h2_w = model.fc2.weight.data.cpu().numpy()\n","h2_b = model.fc2.bias.data.cpu().numpy() \n","\n","# Weights histogram\n","fig, axs = plt.subplots(4, 1, figsize=(12,8))\n","axs[0].hist(c1_w.flatten(), 50)\n","axs[0].set_title('First convolutional layer weights')\n","axs[1].hist(c2_w.flatten(), 50)\n","axs[1].set_title('Second convolutional layer weights')\n","axs[2].hist(h1_w.flatten(), 50)\n","axs[2].set_title('First fully connected layer weights')\n","axs[3].hist(h2_w.flatten(), 50)\n","axs[3].set_title('Second fully connected layer weights')\n","[ax.grid() for ax in axs]\n","plt.tight_layout()\n","plt.savefig('Weights_Class.pdf', bbox_inches='tight')\n","files.download('Weights_Class.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZk6q6AYOoBS"},"source":["## Activation Profiles"]},{"cell_type":"code","metadata":{"id":"GgLbQsbxOoM5"},"source":["def get_activation(layer, input, output):\n","    global activation\n","    activation = torch.relu(output) \n","\n","### Register hook  \n","hook_handle = model.fc2.register_forward_hook(get_activation)\n","\n","### Analyze activations\n","model = model.to(device)\n","model.eval()\n","tloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n","with torch.no_grad():\n","    for sample_batched in tloader:\n","        x1 = sample_batched[0].to(device)\n","        label1 = sample_batched[1].to(device)\n","        break\n","    y1 = model(x1)\n","    z1 = activation\n","    for sample_batched in tloader:\n","        x2 = sample_batched[0].to(device)\n","        label2 = sample_batched[1].to(device)\n","        break\n","    y2 = model(x2)\n","    z2 = activation\n","    for sample_batched in tloader:\n","        x3 = sample_batched[0].to(device)\n","        label3 = sample_batched[1].to(device)\n","        break\n","    y3 = model(x3)\n","    z3 = activation\n","\n","### Remove hook\n","hook_handle.remove()\n","\n","### Plot activations\n","fig, axs = plt.subplots(3, 1, figsize=(12,8))\n","axs[0].stem(z1.cpu().numpy()[0], use_line_collection=True)\n","axs[0].set_title('Label = %d' % label1.cpu().numpy())\n","axs[1].stem(z2.cpu().numpy()[0], use_line_collection=True)\n","axs[1].set_title('Label = %d' % label2.cpu().numpy())\n","axs[2].stem(z3.cpu().numpy()[0], use_line_collection=True)\n","axs[2].set_title('Label = %d' % label3.cpu().numpy())\n","plt.tight_layout()\n","plt.savefig('Activations_Class.pdf', bbox_inches='tight')\n","files.download('Activations_Class.pdf')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGM2IRCXO4g5"},"source":["## Receptive fields"]},{"cell_type":"code","metadata":{"id":"hCXk1z4uPDnF"},"source":["example = DataLoader(test_dataset, batch_size=1, shuffle=True) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26YZvBcGTCeM"},"source":["with torch.no_grad(): \n","    for sample_batched in (example):\n","        x_batch = sample_batched[0].to(device)\n","        first_conv = model.forward(x_batch, 1)\n","        first_conv = first_conv.cpu().numpy()\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0n4kEfSRTCpK"},"source":["fig, axs = plt.subplots(2, 5, figsize=(15,5))\n","axs = axs.flatten()\n","for i in range(5):\n","    axs[i].imshow(c1_w[i, 0, :, :], cmap='Greys')\n","    axs[i].set_yticks([])\n","    axs[i].set_xticks([])\n","for i in range(5):\n","    axs[i+5].imshow(first_conv[0, i, :, :], cmap='Greys')\n","    axs[i+5].set_yticks([])\n","    axs[i+5].set_xticks([])\n","\n","plt.savefig('Filters1_Class.pdf', bbox_inches='tight')\n","files.download('Filters1_Class.pdf')\n","plt.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZCTqIJeTCvD"},"source":[" with torch.no_grad(): \n","    for sample_batched in (example):\n","        x_batch = sample_batched[0].to(device)\n","        second_conv = model.forward(x_batch, 2)\n","        second_conv = second_conv.cpu().numpy()\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRXH97uAVC24"},"source":["fig, axs = plt.subplots(2, 5, figsize=(15,5))\n","axs = axs.flatten()\n","for i in range(5):\n","    axs[i].imshow(c2_w[i, 0, :, :], cmap='Greys')\n","    axs[i].set_yticks([])\n","    axs[i].set_xticks([])\n","for i in range(5):\n","    axs[i+5].imshow(second_conv[0, i, :, :], cmap='Greys')\n","    axs[i+5].set_yticks([])\n","    axs[i+5].set_xticks([])\n","\n","plt.savefig('Filters2_Class.pdf', bbox_inches='tight')\n","files.download('Filters2_Class.pdf')\n","plt.plot()"],"execution_count":null,"outputs":[]}]}