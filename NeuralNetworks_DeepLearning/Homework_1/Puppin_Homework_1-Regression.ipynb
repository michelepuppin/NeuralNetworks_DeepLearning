{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Puppin_Homework_1-Regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gwv0QOeVLlN2"},"source":["#NEURAL NETWORKS AND DEEP LEARNING\n","\n","## Homework 1 - Supervised Deep Learning\n","\n","### Regression task\n","\n","Puppin Michele - 1227474"]},{"cell_type":"code","metadata":{"id":"zYT110tEMZRs"},"source":["# Import packages\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","\n","# Set the seed\n","np.random.seed(25)\n","torch.manual_seed(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OW_a_e5dP04j"},"source":["# Set device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Training device: {device}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxrsrLPsPn_R"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"InH8YxG8QQtZ"},"source":["# Load and transform dataset\n","class CsvDataset(Dataset):\n","\n","  def __init__(self, csv_file, transform=None):\n","    self.transform = transform\n","    self.data = pd.read_csv(csv_file)\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, idx):\n","    sample = (self.data.iloc[idx]['input'], self.data.iloc[idx]['label'])\n","    if self.transform:\n","        sample = self.transform(sample)\n","    return sample\n","\n","# Convert sample to Tensors\n","class ToTensor(object):\n","    def __call__(self, sample):\n","        x, y = sample\n","        return (torch.tensor([x]).float(),\n","                torch.tensor([y]).float())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8boA2DOfQJyX"},"source":["# Download dataset\n","!wget -P regression_dataset https://gitlab.dei.unipd.it/gadaleta/nnld-2020-21-lab-resources/-/raw/master/homework_1_regression_dataset/train_data.csv\n","!wget -P regression_dataset https://gitlab.dei.unipd.it/gadaleta/nnld-2020-21-lab-resources/-/raw/master/homework_1_regression_dataset/test_data.csv "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-VyNhuWQQoH"},"source":["# Read dataset\n","train_dataset = CsvDataset('regression_dataset/train_data.csv', transform = transforms.Compose([ToTensor()]))\n","test_dataset  = CsvDataset('regression_dataset/test_data.csv',  transform = transforms.Compose([ToTensor()]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9C093R-UHl6"},"source":["## Early stopping"]},{"cell_type":"code","metadata":{"id":"Uh3Tpdr0UG46"},"source":["# Early stopping class definition\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mL--2kqjRpV5"},"source":["## Network definition"]},{"cell_type":"code","metadata":{"id":"li4AsSonQQlY"},"source":["class Net(nn.Module):\n","    \n","    def __init__(self, Ni, Nh1, Nh2, No, DropProb = 0.2):\n","        super().__init__()\n","        self.fc1 = nn.Linear(in_features = Ni, out_features = Nh1)\n","        self.fc2 = nn.Linear(in_features = Nh1, out_features = Nh2)\n","        self.out = nn.Linear(in_features = Nh2, out_features = No)\n","        self.drp = nn.Dropout(p = DropProb)\n","        self.act = nn.ReLU()\n","        print('Network initialized')\n","        \n","    def forward(self, x):\n","        x = self.act(self.fc1(x))\n","        x = self.drp(x)\n","        x = self.act(self.fc2(x))\n","        x = self.drp(x)\n","        x = self.out(x)\n","        return x\n","    \n","    def train_nn(self, train_loader, optimizer, loss_func, device):\n","        train_loss= []\n","        self.train()\n","        for sample_batched in train_loader:\n","            x_batch = sample_batched[0].to(device)\n","            label_batch = sample_batched[1].to(device)\n","            out = self.forward(x_batch)\n","            loss = loss_func(out, label_batch)\n","            self.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_batch = loss.detach().cpu().numpy()\n","            train_loss.append(loss_batch)\n","        return train_loss\n","    \n","    def validation_nn(self, val_loader, loss_func, device):\n","        val_loss = []\n","        self.eval() \n","        with torch.no_grad():\n","            for sample_batched in val_loader:\n","                x_batch = sample_batched[0].to(device)\n","                label_batch = sample_batched[1].to(device)\n","                out = self.forward(x_batch)\n","                loss = loss_func(out, label_batch)\n","                loss_batch = loss.detach().cpu().numpy()\n","                val_loss.append(loss_batch)\n","        return val_loss\n","    \n","    def fit(self, train_loader, val_loader, optimizer, loss_func, epochs, device):\n","        train_loss_log = []\n","        val_loss_log = []\n","\n","        early_stopping = EarlyStopping(patience = 25, verbose = False)\n","\n","        for epoch in range(epochs):\n","            # Training\n","            train_loss = self.train_nn(train_loader, optimizer, loss_func, device)\n","            train_loss_log.append(np.mean(train_loss))\n","            # Validation\n","            val_loss = self.validation_nn(val_loader, loss_func, device)\n","            val_loss_log.append(np.mean(val_loss))\n","\n","            early_stopping(np.mean(val_loss), self)\n","            if early_stopping.early_stop:\n","                print(\"Early stopping\")\n","                break\n","            \n","        return train_loss_log, val_loss_log\n","\n","    def test(self, test_loader, loss_func, device):\n","        test_loss = []\n","        self.eval() \n","        with torch.no_grad(): \n","            for sample_batched in test_loader:\n","                x_batch = sample_batched[0].to(device)\n","                label_batch = sample_batched[1][0].to(device)\n","                out = self.forward(x_batch)\n","                loss = loss_func(out, label_batch)\n","                loss_batch = loss.detach().cpu().numpy()\n","                test_loss.append(loss_batch)\n","        return np.mean(test_loss)\n","\n","    def predict(self, input_loader, device):\n","        output = []\n","        self.eval()\n","        with torch.no_grad(): \n","            for sample_batched in input_loader:\n","                x_batch = sample_batched[0].to(device)\n","                out = self.forward(x_batch)\n","                out = out.cpu().numpy()\n","                output.append(out)\n","        return output\n","\n","    def save(self, path):\n","        torch.save(self.state_dict(), path)\n","        \n","    def load(self, path):\n","        self.load_state_dict( torch.load(path) )\n","        \n","    def restart(self):\n","        self.__init__()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NvVrni1sV66G"},"source":["## K-fold Cross Validation"]},{"cell_type":"code","metadata":{"id":"aa28crhwQQfZ"},"source":["# Divide dataset in K folds\n","def Kfolds_divider(dataset, Kfold=4 ):\n","\n","    n = len(dataset)\n","    folds_len = n // Kfold\n","    folds_idx = np.array([ [j for j in range(i*folds_len,(i+1)*folds_len ) ] for i in range(Kfold)  ])\n","    folds = [ Subset(dataset, folds_idx[i]) for i in range(Kfold)]\n","\n","    return folds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfStnu3YQQcH"},"source":["# CrossValidation\n","def CrossValidation(config, dataset, Kfold, rep, device):\n","\n","    par_log = []\n","    train_loss_log = []\n","    val_loss_log = []\n","\n","    folds = Kfolds_divider(dataset, Kfold)\n","\n","    for i in range(rep):\n","        print(i)\n","        for j in range(len(folds)):\n","\n","            train_set = folds[:j]+folds[j+1:]\n","            train_set = torch.utils.data.dataset.ConcatDataset( train_set )\n","            train_load = DataLoader(train_set, batch_size=20, shuffle=True,  num_workers=0)\n","            val_load   = DataLoader(folds[j],  batch_size=20, shuffle=False, num_workers=0)\n","            \n","            # Random parameter selection\n","            sample_params = {}\n","\n","            for k in config.keys():\n","                sample_params[k] = np.random.choice(config[k])\n","\n","            par_log.append(sample_params)\n","\n","            Ni  = 1\n","            Nh1 = sample_params['Nh']\n","            Nh2 = sample_params['Nh']*2\n","            No  = 1\n","            DropProb = sample_params['Dropout']\n","\n","            model = Net(Ni, Nh1, Nh2, No, DropProb).to(device)\n","\n","            loss_func = nn.MSELoss() \n","            epochs = sample_params['Epochs']\n","\n","            if sample_params['Optimizer']=='Adam':\n","                  opt = optim.Adam(model.parameters(), lr = sample_params['LearningRate'], weight_decay = sample_params['Regularization'])\n","            if sample_params['Optimizer']=='SGD':\n","                  opt = optim.SGD( model.parameters(), lr = sample_params['LearningRate'], weight_decay = sample_params['Regularization'], momentum=0.9)\n","\n","            # Training & validation\n","            train_loss, val_loss = model.fit(train_load, val_load, opt, loss_func, epochs, device)\n","            \n","            # Storing train/loss validation\n","            train_loss_log.append( train_loss )\n","            val_loss_log.append( val_loss )\n","\n","    return par_log, train_loss_log, val_loss_log    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfQ0ACvXXdeq"},"source":["## Training and Testing the network"]},{"cell_type":"markdown","metadata":{"id":"tDvcCbKmXl1S"},"source":["### Model selection"]},{"cell_type":"code","metadata":{"id":"9NVu9cbpQQiX"},"source":["# Define parameters range\n","dict_params = {\n","            'Nh'              : [16, 32, 64, 128],\n","            'LearningRate'    : [0.1, 0.01, 0.001, 0.0001],\n","            'Regularization'  : [1e-3, 1e-4, 1e-5, 1e-6],\n","            'Dropout'         : [0, 0.10, 0.15, 0.20],\n","            'Epochs'          : [1000],\n","            'Optimizer'       : ['SGD', 'Adam']\n","         }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5pG6XLSQQZJ"},"source":["params_list, train_loss_list, val_loss_list = CrossValidation(dict_params, train_dataset, 4, 20, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZIXDlaHvQQWH"},"source":["# Select best parameters\n","best_params = params_list[np.argmin([v[-1] for v in val_loss_list])]\n","best_params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yCrl_oNbF28"},"source":["### Train with best parameters"]},{"cell_type":"code","metadata":{"id":"PWeZqt2zmllJ"},"source":["best_params = {\n","            'Nh'              : 128,\n","            'LearningRate'    : 0.0001,\n","            'Regularization'  : 1e-5,\n","            'Dropout'         : 0.15,\n","            'Epochs'          : 1000,\n","            'Optimizer'       : 'Adam'\n","         }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbJLk0Q_QQTX"},"source":["train_load = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0 )\n","val_load = train_load\n","\n","Ni  = 1\n","Nh1 = best_params['Nh']\n","Nh2 = best_params['Nh']*2\n","No  = 1\n","DropProb = best_params['Dropout']\n","\n","model = Net(Ni, Nh1, Nh2, No, DropProb).to(device)\n","\n","loss_func = nn.MSELoss() \n","epochs = best_params['Epochs']\n","\n","if best_params['Optimizer']=='Adam':\n","        opt = optim.Adam(model.parameters(), lr = best_params['LearningRate'], weight_decay = best_params['Regularization'])\n","if best_params['Optimizer']=='SGD':\n","        opt = optim.SGD( model.parameters(), lr = best_params['LearningRate'], weight_decay = best_params['Regularization'], momentum=0.9)\n","\n","# Training & validation\n","train_loss, val_loss = model.fit(train_load, val_load, opt, loss_func, epochs, device)\n","\n","print('Training loss:', train_loss[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gk2Sz55BQQQY"},"source":["# Plot Training and Validation loss\n","plt.plot(train_loss, label='Training')\n","plt.plot(val_loss, label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.savefig('TrainValLoss_Reg.pdf', bbox_inches='tight')\n","files.download('TrainValLoss_Reg.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3ag7tVjQQHc"},"source":["# Save trained model\n","model.save('net_reg_parameters.torch')\n","torch.save(opt.state_dict(), 'optimizer_reg_state.torch')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVFdaNJk_2si"},"source":["### Test the trained model"]},{"cell_type":"code","metadata":{"id":"-ZAGYCLwnc3s"},"source":["test_load = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0 )\n","\n","# Initialization of the net\n","Ni  = 1\n","Nh1 = best_params['Nh']\n","Nh2 = best_params['Nh']*2\n","No  = 1\n","DropProb = best_params['Dropout']\n","\n","net = Net(Ni, Nh1, Nh2, No, DropProb).to(device)\n","\n","model.load('net_reg_parameters.torch')\n","\n","loss_func = nn.MSELoss() \n","\n","test_loss = model.test(test_load, loss_func, device)\n","\n","print('Test loss:', test_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DXsVaSEpnsR4"},"source":["### Plot predictions"]},{"cell_type":"code","metadata":{"id":"fjdQpbPgnySF"},"source":["# Prepare input values\n","x_vec = np.linspace(-5, 5, 1000).reshape(1000, 1)\n","lb = np.ones(1000).reshape(1000, 1)\n","x = np.hstack((x_vec, lb)).reshape(1000, 2)\n","x = pd.DataFrame(x, columns=['input', 'label'])\n","x.to_csv('pred_data.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l60YBHRen4AU"},"source":["pred_dataset = CsvDataset('/content/pred_data.csv', transform = transforms.Compose([ToTensor()]))\n","pred_loader = DataLoader(pred_dataset, batch_size=20, shuffle=False, num_workers=2)\n","\n","# Run the network to get predictions\n","y_vec = np.array(model.predict(pred_loader, device)).flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kUinvLvn4EY"},"source":["train_df = pd.read_csv('regression_dataset/train_data.csv')\n","test_df = pd.read_csv('regression_dataset/test_data.csv')\n","\n","# Plot output\n","plt.figure(figsize=(12,8))\n","plt.plot(x['input'], y_vec, 'g--', label='Network output')\n","plt.scatter(train_df['input'], train_df['label'], label='Training set')\n","plt.scatter(test_df['input'], test_df['label'], label='Test set')\n","plt.xlabel('Input', fontsize=20)\n","plt.ylabel('Label', fontsize=20)\n","plt.grid(ls='--')\n","plt.legend(fontsize=18)\n","plt.tick_params(labelsize=16)\n","\n","plt.savefig('Predictions_Reg.pdf', bbox_inches='tight')\n","files.download('Predictions_Reg.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqooGFe4okeU"},"source":["## Weights Histogram"]},{"cell_type":"code","metadata":{"id":"0EVSDaDWn4HD"},"source":["# First hidden layer\n","h1_w = model.fc1.weight.data.cpu().numpy() \n","h1_b = model.fc1.bias.data.cpu().numpy() \n","\n","# Second hidden layer\n","h2_w = model.fc2.weight.data.cpu().numpy()\n","h2_b = model.fc2.bias.data.cpu().numpy() \n","\n","# Output layer\n","out_w = model.out.weight.data.cpu().numpy() \n","out_b = model.out.bias.data.cpu().numpy() \n","\n","# Weights histogram\n","fig, axs = plt.subplots(3, 1, figsize=(12,8))\n","axs[0].hist(h1_w.flatten(), 50)\n","axs[0].set_title('First hidden layer weights')\n","axs[1].hist(h2_w.flatten(), 50)\n","axs[1].set_title('Second hidden layer weights')\n","axs[2].hist(out_w.flatten(), 50)\n","axs[2].set_title('Output layer weights')\n","[ax.grid() for ax in axs]\n","plt.tight_layout()\n","plt.savefig('Weights_Reg.pdf', bbox_inches='tight')\n","files.download('Weights_Reg.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ve8toYDYor0-"},"source":["## Activation Profiles"]},{"cell_type":"code","metadata":{"id":"MuMYWqHPosK2"},"source":["def get_activation(layer, input, output):\n","    global activation\n","    activation = torch.relu(output) \n","\n","### Register hook  \n","hook_handle = model.fc2.register_forward_hook(get_activation)\n","\n","### Analyze activations\n","model = model.to(device)\n","model.eval()\n","with torch.no_grad():\n","    x1 = torch.tensor([-5.0]).float().to(device)\n","    y1 = model(x1)\n","    z1 = activation\n","    x2 = torch.tensor([0.0]).float().to(device)\n","    y2 = model(x2)\n","    z2 = activation\n","    x3 = torch.tensor([5.0]).float().to(device)\n","    y3 = model(x3)\n","    z3 = activation\n","\n","### Remove hook\n","hook_handle.remove()\n","\n","### Plot activations\n","fig, axs = plt.subplots(3, 1, figsize=(12,8))\n","axs[0].stem(z1.cpu().numpy(), use_line_collection=True)\n","axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n","axs[1].stem(z2.cpu().numpy(), use_line_collection=True)\n","axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n","axs[2].stem(z3.cpu().numpy(), use_line_collection=True)\n","axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n","plt.tight_layout()\n","plt.savefig('Activations_Reg.pdf', bbox_inches='tight')\n","files.download('Activations_Reg.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]}]}