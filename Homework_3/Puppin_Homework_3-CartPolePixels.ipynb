{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Puppin_Homework_3-CartPolePixels.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2OZFKEqBpb3k"},"source":["#NEURAL NETWORKS AND DEEP LEARNING\n","\n","## Homework 3 - Deep Reinforcement Learning\n","\n","### Cart-Pole environment using pixels\n","\n","Puppin Michele - 1227474"]},{"cell_type":"code","metadata":{"id":"2ETBlT7op4yH"},"source":["# Install libraries\n","!pip install gym\n","!apt update\n","!apt-get install python-opengl -y\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKrvdr_0phaH"},"source":["# Import packages\n","import random\n","import torch\n","import numpy as np\n","import gym\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from google.colab import files\n","import cv2\n","\n","import torch\n","from torch import nn\n","from collections import deque # this python module implements exactly what we need for the replay memeory\n","\n","import glob\n","import io\n","import base64\n","import os\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display\n","from gym.wrappers import Monitor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MrnfaTvpTwjc"},"source":["# Set device\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f\"Training device: {device}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kByO8FDIqYcD"},"source":["# Enable gym environment rendering in Colab"]},{"cell_type":"code","metadata":{"id":"FGR9IFBpqQte"},"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","def show_videos():\n","  mp4list = glob.glob('video/*.mp4')\n","  mp4list.sort()\n","  for mp4 in mp4list:\n","    print(f\"\\nSHOWING VIDEO {mp4}\")\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    \n","def wrap_env(env, video_callable=None):\n","  env = Monitor(env, './video', force=True, video_callable=video_callable)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uc946Frpqidg"},"source":["# Experience replay (Replay Memory)"]},{"cell_type":"code","metadata":{"id":"qQp8BtybpqU_"},"source":["class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n","\n","    def push(self, state, action, next_state, reward):\n","        self.memory.append( (state, action, next_state, reward) ) # Add the tuple (state, action, next_state, reward) to the queue\n","\n","    def sample(self, batch_size):\n","        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n","        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n","\n","    def __len__(self):\n","        return len(self.memory) # Return the number of samples currently stored in the memory"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8jjNkLAtqsn-"},"source":["## Network definition"]},{"cell_type":"code","metadata":{"id":"PSFc6cP8q1O9"},"source":["class DQN(nn.Module):\n","\n","    def __init__(self, channels, lin_input, action_space_dim):\n","        super().__init__()\n","        \n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels = channels, out_channels = 64, kernel_size = 4, stride = 2, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            )\n","                \n","        self.linear = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(lin_input, 128),\n","            nn.Tanh(),\n","            nn.Linear(128, 128),\n","            nn.Tanh(),\n","            nn.Linear(128, action_space_dim)\n","            )\n","        \n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return self.linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUYRexBwqsqt"},"source":["## Exploration Policy"]},{"cell_type":"code","metadata":{"id":"Vt6O38kDqqN3"},"source":["# Epsilon-greedy policy\n","def choose_action_epsilon_greedy(net, state, epsilon):\n","    \n","    if epsilon > 1 or epsilon < 0:\n","        raise Exception('The epsilon value must be between 0 and 1')\n","                \n","    # Evaluate the network output from the current state\n","    with torch.no_grad():\n","        net.eval()\n","        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n","        net_out = net(state.to(device)).cpu()\n","\n","    # Get the best action (argmax of the network output)\n","    best_action = int(net_out.argmax())\n","    # Get the number of possible actions\n","    action_space_dim = net_out.shape[-1]\n","\n","    # Select a non optimal action with probability epsilon, otherwise choose the best action\n","    if random.random() < epsilon:\n","        # List of non-optimal actions\n","        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n","        # Select randomly\n","        action = random.choice(non_optimal_actions)\n","    else:\n","        # Select best action\n","        action = best_action\n","        \n","    return action, net_out.numpy()\n","\n","# Softmax policy\n","def choose_action_softmax(net, state, temperature):\n","    \n","    if temperature < 0:\n","        raise Exception('The temperature value must be greater than or equal to 0 ')\n","        \n","    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n","    if temperature == 0:\n","        return choose_action_epsilon_greedy(net, state, 0)\n","    \n","    # Evaluate the network output from the current state\n","    with torch.no_grad():\n","        net.eval()\n","        state = torch.tensor(state, dtype=torch.float32)\n","        net_out = net(state.to(device)).cpu()\n","\n","    # Apply softmax with temp\n","    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n","    softmax_out = nn.functional.softmax(net_out[0] / temperature, dim=0).numpy()\n","                \n","    # Sample the action using softmax output as mass pdf\n","    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n","    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n","    \n","    return action, net_out.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X05aEMj_r_NI"},"source":["### Create environment\n","env = gym.make('CartPole-v1') # Initialize the Gym environment\n","env.seed(0) # Set a random seed for the environment (reproducible results)\n","\n","# Get the shapes of the state space (observation_space) and action space (action_space)\n","state_space_dim = env.observation_space.shape[0]\n","action_space_dim = env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2r3G1HxsX3T"},"source":["## Update function"]},{"cell_type":"code","metadata":{"id":"JLXm6S2FsT6d"},"source":["def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n","        \n","    # Sample the data from the replay memory\n","    batch = replay_mem.sample(batch_size)\n","    batch_size = len(batch)\n","\n","    # Create tensors for each element of the batch\n","    #states      = torch.tensor([s[0][0] for s in batch], dtype=torch.float32)\n","    states      = torch.stack([s[0][0] for s in batch])\n","    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n","    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n","\n","    # Compute a mask of non-final states (all the elements where the next state is not None)\n","    #non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended\n","    non_final_next_states = torch.stack([s[2][0] for s in batch if s[2] is not None]) # the next state can be None if the game has ended\n","    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n","\n","    # Compute all the Q values (forward pass)\n","    policy_net.train()\n","    q_values = policy_net(states.to(device)).cpu()\n","    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n","    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n","\n","    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n","    with torch.no_grad():\n","      target_net.eval()\n","      q_values_target = target_net(non_final_next_states.to(device)).cpu()\n","    next_state_max_q_values = torch.zeros(batch_size)\n","    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n","\n","    # Compute the expected Q values\n","    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n","    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n","\n","    # Compute the Huber loss\n","    loss = loss_fn(state_action_values, expected_state_action_values)\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n","    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_DhlIfu8lW-"},"source":["### Image extraction"]},{"cell_type":"code","metadata":{"id":"G6nny5AZ8kxc"},"source":["def image_extraction(screen):\n","    screen = screen[170:290,:,2] # Crop the image and select one colour channel\n","    screen = cv2.resize(screen, (200, 100), interpolation=cv2.INTER_CUBIC)\n","    screen = screen[np.newaxis,:]\n","    screen = torch.tensor(screen.copy(), dtype=torch.float32)\n","    return screen.unsqueeze(0) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ho4zL1Fvtw4W"},"source":["## Train with best parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"T-XvrNeK8p8s","executionInfo":{"status":"ok","timestamp":1630076092660,"user_tz":-120,"elapsed":1474,"user":{"displayName":"Michele Puppin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRS_KXhTLE8H-EFQyu3ENF4tW0st3lMob2BMsA=s64","userId":"10648899063932926796"}},"outputId":"0c60912e-d8bd-40f9-b57a-dd82ee68b7db"},"source":["# Initialize the Gym environment\n","env = gym.make('CartPole-v1') \n","env.seed(0) # Set a random seed for the environment (reproducible results)\n","\n","env.reset()\n","screen = env.render(mode='rgb_array')\n","screen = image_extraction(screen)\n","env.close()\n","\n","print(screen.shape)\n","plt.imshow(screen.cpu().squeeze().numpy(), cmap = 'gray')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 1, 100, 200])\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"2JXR_Cd1jjAo"},"source":["best_params = {'beta': 15,\n","          'gamma': 0.98,            \n","          'lr': 1e-1,          \n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbooYt7hyKj7"},"source":["### Define exploration profile\n","initial_value = 5\n","num_iterations = 1000\n","exp_decay = np.exp(-np.log(initial_value) / num_iterations * best_params['beta']) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n","exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1Y0Hwu43E9-"},"source":["### PARAMETERS\n","gamma = best_params['gamma']   # gamma parameter for the long term reward\n","replay_memory_capacity = 10000   # Replay memory capacity\n","lr = best_params['lr']   # Optimizer learning rate\n","target_net_update_steps = 10   # Number of episodes to wait before updating the target network\n","batch_size = 128   # Number of samples to take from the replay memory for each update\n","bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n","min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH7RnRKoyOSA"},"source":["# Set random seeds\n","torch.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","channels = 1\n","lin_input = 30912\n","\n","### Initialize the replay memory\n","replay_mem = ReplayMemory(replay_memory_capacity)    \n","\n","### Initialize the policy network\n","policy_net = DQN(channels, lin_input, action_space_dim).to(device)\n","\n","### Initialize the target network with the same weights of the policy network\n","target_net = DQN(channels, lin_input, action_space_dim).to(device)\n","target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n","\n","### Initialize the optimizer\n","optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n","\n","### Initialize the loss function (Huber loss)\n","loss_fn = nn.SmoothL1Loss()\n","\n","# Initialize the Gym environment\n","env = gym.make('CartPole-v1') \n","env.seed(0) # Set a random seed for the environment (reproducible results)\n","\n","# This is for creating the output video in Colab, not required outside Colab\n","#env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes\n","\n","final_score_log = []\n","\n","for episode_num, tau in enumerate(tqdm(exploration_profile)):\n","\n","    # Reset the environment and get the initial state\n","    state = env.reset()\n","    screen = image_extraction(env.render(mode='rgb_array'))\n","\n","    # Reset the score. The final score will be the total amount of steps before the pole falls\n","    score = 0\n","    done = False\n","\n","    # Go on until the pole falls off\n","    while not done:\n","\n","      # Choose the action following the policy\n","      action, q_values = choose_action_softmax(policy_net, screen, temperature=tau)\n","      \n","      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n","      next_state, reward, done, info = env.step(action)\n","      next_screen = image_extraction(env.render(mode='rgb_array'))\n","\n","      # We apply a (linear) penalty when the cart is far from center\n","      pos_weight = 1\n","      reward = reward - pos_weight * np.abs(state[0]) \n","\n","      # Update the final score (+1 for each step)\n","      score += 1\n","\n","      # Apply penalty for bad state\n","      if done: # if the pole has fallen down \n","          reward += bad_state_penalty\n","          next_state = None\n","          next_screen = None \n","\n","      if type(next_state) != \"NoneType\":\n","          next_screen = image_extraction(env.render(mode='rgb_array'))\n","\n","      # Update the replay memory\n","      replay_mem.push(screen, action, next_screen, reward)\n","\n","      # Update the network\n","      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n","          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n","\n","      # Visually render the environment (disable to speed up the training)\n","      #env.render()\n","\n","      # Set the current state for the next iteration\n","      state = next_state\n","      screen = next_screen\n","\n","    # Update the target network every target_net_update_steps episodes\n","    if episode_num % target_net_update_steps == 0:\n","        #print('Updating target network...')\n","        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n","\n","    # Print the final score\n","    #print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}\") # Print the final score\n","    final_score_log.append(score)\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55DNDoFazYsI"},"source":["# Plot scores\n","plt.figure(figsize=(12,8))\n","plt.plot(final_score_log)\n","plt.grid()\n","plt.xlabel('Iteration')\n","plt.ylabel('Scores')\n","\n","plt.savefig('Score_CPsp.pdf', bbox_inches='tight')\n","files.download('Score_CPsp.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2qjRYCVdWb-"},"source":["### Multiple frames"]},{"cell_type":"code","metadata":{"id":"KIBwm7GHpmkE"},"source":["# Softmax policy\n","def choose_action_softmax(net, state, temperature):\n","    \n","    if temperature < 0:\n","        raise Exception('The temperature value must be greater than or equal to 0 ')\n","        \n","    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n","    if temperature == 0:\n","        return choose_action_epsilon_greedy(net, state, 0)\n","    \n","    # Evaluate the network output from the current state\n","    with torch.no_grad():\n","        net.eval()\n","        state = torch.tensor(state, dtype=torch.float32)\n","        net_out = net(state.to(device)).cpu().squeeze(0)\n","\n","    # Apply softmax with temp\n","    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n","    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()\n","                \n","    # Sample the action using softmax output as mass pdf\n","    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n","    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n","    \n","    return action, net_out.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzU5U3BGwBgP"},"source":["def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n","        \n","    # Sample the data from the replay memory\n","    batch = replay_mem.sample(batch_size)\n","    batch_size = len(batch)\n","\n","    # Create tensors for each element of the batch\n","    states      = torch.from_numpy(np.array( [s[0] for s in batch], dtype=np.float32 ))\n","    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n","    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n","\n","    # Compute a mask of non-final states (all the elements where the next state is not None)\n","    non_final_next_states =  torch.from_numpy(np.array([s[2] for s in batch if s[2] is not None], dtype=np.float32)) # the next state can be None if the game has ended\n","    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n","\n","    # Compute all the Q values (forward pass)\n","    policy_net.train()\n","    q_values = policy_net(states.to(device)).cpu()\n","    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n","    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n","\n","    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n","    with torch.no_grad():\n","      target_net.eval()\n","      q_values_target = target_net(non_final_next_states.to(device)).cpu()\n","    next_state_max_q_values = torch.zeros(batch_size)\n","    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n","\n","    # Compute the expected Q values\n","    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n","    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n","\n","    # Compute the Huber loss\n","    loss = loss_fn(state_action_values, expected_state_action_values)\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n","    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6EoPsu54ZXd"},"source":["def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n","    # Sample the data from the replay memory\n","    batch = replay_mem.sample(batch_size)\n","    batch_size = len(batch)\n","    # Create tensors for each element of the batch\n","    states  = np.array( [s[0] for s in batch], dtype=np.float32 )\n","    states = torch.from_numpy(states)\n","    actions = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n","    rewards = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n","    # Compute a mask of non-final states (all the elements where the next state is not None)\n","    non_final_next_states =  np.array([s[2] for s in batch if s[2] is not None], dtype=np.float32)# the next state can be None if the game has ended\n","    non_final_next_states = torch.from_numpy(non_final_next_states)\n","    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n","    # Compute all the Q values (forward pass)\n","    policy_net.train()\n","    q_values = policy_net(states.to(device)).cpu()\n","    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n","    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n","    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n","    with torch.no_grad():\n","        target_net.eval()\n","        q_values_target = target_net(non_final_next_states.to(device)).cpu()\n","    next_state_max_q_values = torch.zeros(batch_size)\n","    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n","    # Compute the expected Q values\n","    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n","    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n","    # Compute the Huber loss\n","    loss = loss_fn(state_action_values, expected_state_action_values)\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n","    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkqR8lRyol6G"},"source":["def image_extraction(screen):\n","    screen = screen[170:290,:,2] # Crop the image and select one colour channel\n","    screen = cv2.resize(screen, (200, 170), interpolation=cv2.INTER_CUBIC)\n","    screen = screen[np.newaxis,:]\n","    return screen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmXMGWU1dWDk"},"source":["# Set random seeds\n","torch.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","channels = 4 \n","lin_input = 57408\n","\n","### Initialize the replay memory\n","replay_mem = ReplayMemory(replay_memory_capacity)    \n","\n","### Initialize the policy network\n","policy_net = DQN(channels, lin_input, action_space_dim).to(device)\n","\n","### Initialize the target network with the same weights of the policy network\n","target_net = DQN( channels, lin_input, action_space_dim).to(device)\n","target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n","\n","### Initialize the optimizer\n","optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n","\n","### Initialize the loss function (Huber loss)\n","loss_fn = nn.SmoothL1Loss()\n","\n","# Initialize the Gym environment\n","env = gym.make('CartPole-v1') \n","env.seed(0) # Set a random seed for the environment (reproducible results)\n","\n","ml = 3\n","screen_history = deque(maxlen=ml)\n","\n","final_score_log = []\n","\n","for i in range(ml): screen_history.append(np.zeros((1, 170, 200) ))\n","\n","for episode_num, tau in enumerate(tqdm(exploration_profile)):\n","    # Reset the environment and get the initial state\n","    state = env.reset()\n","    screen = image_extraction(env.render(mode='rgb_array'))\n","    # Reset the score. The final score will be the total amount of steps before the pole falls\n","    score = 0\n","    done = False\n","    # Go on until the pole falls off\n","    for i in range(ml):\n","        screen = np.append( screen, screen_history[-i-1], axis=0)\n","    st = 0\n","    while not done:\n","        # Adding history\n","        for i in range(ml):\n","            if screen_history[i] is None: screen_history[i] = np.zeros((1, 170, 200) )\n","        \n","        screen_history.append(screen[0][np.newaxis, :])\n","        \n","        # Choose the action following the policy\n","        action, q_values = choose_action_softmax(policy_net, torch.tensor(screen).type(torch.float).unsqueeze(0), temperature=tau)\n","\n","        # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n","        next_state, reward, done, info = env.step(action)\n","        next_screen = image_extraction(env.render(mode='rgb_array'))\n","        \n","        for i in range(ml):\n","            next_screen = np.append( next_screen, screen_history[-i-1], axis=0) \n","        \n","        # We apply a (linear) penalty when the cart is far from center\n","        pos_weight = 1\n","        reward = reward - pos_weight * np.abs(state[0]) \n","\n","        # Update the final score (+1 for each step)\n","        score += 1\n","        # Apply penalty for bad state\n","        if done: # if the pole has fallen down \n","            reward += bad_state_penalty\n","            next_state = None\n","            next_screen = None\n","\n","        # Update the replay memory\n","        replay_mem.push(screen, action, next_screen, reward)\n","\n","        # Update the network\n","        if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n","            update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n","\n","        # Set the current state for the next iteration\n","        screen = next_screen\n","        state = next_state\n","\n","    # Update the target network every target_net_update_steps episodes\n","    if episode_num % target_net_update_steps == 0:\n","        #print('Updating target network...')\n","        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n","\n","    # Print the final score\n","    #print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}\") # Print the final score\n","    final_score_log.append(score)\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_XJNW0ue_bm"},"source":["# Plot scores\n","plt.figure(figsize=(12,8))\n","plt.plot(final_score_log)\n","plt.grid()\n","plt.xlabel('Iteration')\n","plt.ylabel('Scores')\n","\n","plt.savefig('Score_CPsp.pdf', bbox_inches='tight')\n","files.download('Score_CPsp.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]}]}