{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Puppin_Homework_3-MountainCar.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2OZFKEqBpb3k"},"source":["#NEURAL NETWORKS AND DEEP LEARNING\n","\n","## Homework 3 - Deep Reinforcement Learning\n","\n","### Mountain-Car environment\n","\n","Puppin Michele - 1227474"]},{"cell_type":"code","metadata":{"id":"2ETBlT7op4yH"},"source":["# Install libraries\n","!pip install gym\n","!apt update\n","!apt-get install python-opengl -y\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKrvdr_0phaH"},"source":["# Import packages\n","import random\n","import torch\n","import numpy as np\n","import gym\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from google.colab import files\n","\n","from torch import nn\n","from collections import deque # this python module implements exactly what we need for the replay memeory\n","\n","import glob\n","import io\n","import base64\n","import os\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display\n","from gym.wrappers import Monitor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kByO8FDIqYcD"},"source":["# Enable gym environment rendering in Colab"]},{"cell_type":"code","metadata":{"id":"FGR9IFBpqQte"},"source":["display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","def show_videos():\n","  mp4list = glob.glob('video/*.mp4')\n","  mp4list.sort()\n","  for mp4 in mp4list:\n","    print(f\"\\nSHOWING VIDEO {mp4}\")\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    \n","def wrap_env(env, video_callable=None):\n","  env = Monitor(env, './video', force=True, video_callable=video_callable)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uc946Frpqidg"},"source":["# Experience replay (Replay Memory)"]},{"cell_type":"code","metadata":{"id":"qQp8BtybpqU_"},"source":["class ReplayMemory(object):\n","\n","    def __init__(self, capacity):\n","        self.memory = deque(maxlen=capacity) # Define a queue with maxlen \"capacity\"\n","\n","    def push(self, state, action, next_state, reward):\n","        self.memory.append( (state, action, next_state, reward) ) # Add the tuple (state, action, next_state, reward) to the queue\n","\n","    def sample(self, batch_size):\n","        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory\n","        return random.sample(self.memory, batch_size) # Randomly select \"batch_size\" samples\n","\n","    def __len__(self):\n","        return len(self.memory) # Return the number of samples currently stored in the memory"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qsa8fEjqnNI"},"source":["# Policy network"]},{"cell_type":"markdown","metadata":{"id":"8jjNkLAtqsn-"},"source":["## Network definition"]},{"cell_type":"code","metadata":{"id":"PSFc6cP8q1O9"},"source":["class DQN(nn.Module):\n","\n","    def __init__(self, state_space_dim, action_space_dim):\n","        super().__init__()\n","\n","        self.linear = nn.Sequential(\n","                nn.Linear(state_space_dim, 128),\n","                nn.Tanh(),\n","                nn.Linear(128, 128),\n","                nn.Tanh(),\n","                nn.Linear(128, action_space_dim)\n","                )\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUYRexBwqsqt"},"source":["## Exploration Policy"]},{"cell_type":"code","metadata":{"id":"Vt6O38kDqqN3"},"source":["# Epsilon-greedy policy\n","def choose_action_epsilon_greedy(net, state, epsilon):\n","    \n","    if epsilon > 1 or epsilon < 0:\n","        raise Exception('The epsilon value must be between 0 and 1')\n","                \n","    # Evaluate the network output from the current state\n","    with torch.no_grad():\n","        net.eval()\n","        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n","        net_out = net(state)\n","\n","    # Get the best action (argmax of the network output)\n","    best_action = int(net_out.argmax())\n","    # Get the number of possible actions\n","    action_space_dim = net_out.shape[-1]\n","\n","    # Select a non optimal action with probability epsilon, otherwise choose the best action\n","    if random.random() < epsilon:\n","        # List of non-optimal actions\n","        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n","        # Select randomly\n","        action = random.choice(non_optimal_actions)\n","    else:\n","        # Select best action\n","        action = best_action\n","        \n","    return action, net_out.numpy()\n","\n","# Softmax policy\n","def choose_action_softmax(net, state, temperature):\n","    \n","    if temperature < 0:\n","        raise Exception('The temperature value must be greater than or equal to 0 ')\n","        \n","    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0\n","    if temperature == 0:\n","        return choose_action_epsilon_greedy(net, state, 0)\n","    \n","    # Evaluate the network output from the current state\n","    with torch.no_grad():\n","        net.eval()\n","        state = torch.tensor(state, dtype=torch.float32)\n","        net_out = net(state)\n","\n","    # Apply softmax with temp\n","    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability\n","    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()\n","                \n","    # Sample the action using softmax output as mass pdf\n","    all_possible_actions = np.arange(0, softmax_out.shape[-1])\n","    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from \"all_possible_actions\" with the probability distribution p (softmax_out in this case)\n","    \n","    return action, net_out.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5r7j045Irz_-"},"source":["# Deep Reinforcement learning for Gym Environment (CartPole-v1)"]},{"cell_type":"code","metadata":{"id":"X05aEMj_r_NI"},"source":["### Create environment\n","env = gym.make('MountainCar-v0') # Initialize the Gym environment\n","env.seed(0) # Set a random seed for the environment (reproducible results)\n","\n","# Get the shapes of the state space (observation_space) and action space (action_space)\n","state_space_dim = env.observation_space.shape[0]\n","action_space_dim = env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2r3G1HxsX3T"},"source":["## Update function"]},{"cell_type":"code","metadata":{"id":"JLXm6S2FsT6d"},"source":["def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):\n","        \n","    # Sample the data from the replay memory\n","    batch = replay_mem.sample(batch_size)\n","    batch_size = len(batch)\n","\n","    # Create tensors for each element of the batch\n","    states      = torch.tensor([s[0] for s in batch], dtype=torch.float32)\n","    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)\n","    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n","\n","    # Compute a mask of non-final states (all the elements where the next state is not None)\n","    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended\n","    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)\n","\n","    # Compute all the Q values (forward pass)\n","    policy_net.train()\n","    q_values = policy_net(states)\n","    # Select the proper Q value for the corresponding action taken Q(s_t, a)\n","    state_action_values = q_values.gather(1, actions.unsqueeze(1))\n","\n","    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )\n","    with torch.no_grad():\n","      target_net.eval()\n","      q_values_target = target_net(non_final_next_states)\n","    next_state_max_q_values = torch.zeros(batch_size)\n","    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]\n","\n","    # Compute the expected Q values\n","    expected_state_action_values = rewards + (next_state_max_q_values * gamma)\n","    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape\n","\n","    # Compute the Huber loss\n","    loss = loss_fn(state_action_values, expected_state_action_values)\n","\n","    # Optimize the model\n","    optimizer.zero_grad()\n","    loss.backward()\n","    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)\n","    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ho4zL1Fvtw4W"},"source":["## Train with best parameters"]},{"cell_type":"code","metadata":{"id":"2JXR_Cd1jjAo"},"source":["best_params = {'beta': 15,\n","          'gamma': 0.98,            \n","          'lr': 1e-1,          \n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbooYt7hyKj7"},"source":["### Define exploration profile\n","initial_value = 5\n","num_iterations = 1000\n","exp_decay = np.exp(-np.log(initial_value) / num_iterations * best_params['beta']) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations\n","exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH7RnRKoyOSA"},"source":["# Set random seeds\n","torch.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","### PARAMETERS\n","gamma = best_params['gamma']   # gamma parameter for the long term reward\n","replay_memory_capacity = 10000   # Replay memory capacity\n","lr = best_params['lr']   # Optimizer learning rate\n","target_net_update_steps = 10   # Number of episodes to wait before updating the target network\n","batch_size = 128   # Number of samples to take from the replay memory for each update\n","bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) \n","min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training\n","\n","### Initialize the replay memory\n","replay_mem = ReplayMemory(replay_memory_capacity)    \n","\n","### Initialize the policy network\n","policy_net = DQN(state_space_dim, action_space_dim)\n","\n","### Initialize the target network with the same weights of the policy network\n","target_net = DQN(state_space_dim, action_space_dim)\n","target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n","\n","### Initialize the optimizer\n","optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network\n","\n","### Initialize the loss function (Huber loss)\n","loss_fn = nn.SmoothL1Loss()\n","\n","# Initialize the Gym environment\n","env = gym.make('MountainCar-v0') \n","env.seed(0) # Set a random seed for the environment (reproducible results)\n","\n","# This is for creating the output video in Colab, not required outside Colab\n","env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes\n","\n","final_score_log = []\n","last_scores = deque()\n","\n","for episode_num, tau in enumerate(tqdm(exploration_profile)):\n","\n","    # Reset the environment and get the initial state\n","    state = env.reset()\n","    # Reset the score. The final score will be the total amount of steps before the pole falls\n","    score = 0\n","    done = False\n","\n","    # Go on until the pole falls off\n","    while not done:\n","\n","      # Choose the action following the policy\n","      action, q_values = choose_action_softmax(policy_net, state, temperature=tau)\n","      \n","      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n","      next_state, reward, done, info = env.step(action)\n","\n","      # We apply a (linear) penalty \n","      pos_weight = 5\n","        \n","      if (action == 0 and state[1] < 0) or (action == 2 and state[1] > 0):\n","          reward += 1\n","      else:\n","          reward -= 1\n","\n","      # The state is equal to the position\n","      score = state[0]\n","\n","      # Update the replay memory\n","      replay_mem.push(state, action, next_state, reward)\n","\n","      # Update the network\n","      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often\n","          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)\n","\n","      # Visually render the environment (disable to speed up the training)\n","      env.render()\n","\n","      # Set the current state for the next iteration\n","      state = next_state\n","\n","    # Update the target network every target_net_update_steps episodes\n","    if episode_num % target_net_update_steps == 0:\n","        #print('Updating target network...')\n","        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network\n","\n","    # Print the final score\n","    #print(f\"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}\") # Print the final score\n","    final_score_log.append(score)\n","\n","    # Early stopping\n","    last_scores.append(score)\n","    if len(last_scores) > 10:\n","        last_scores.popleft()\n","    if sum(last_scores)/10 >= 0.48: # If the last 10 steps reaches the maximum score it stops the training\n","        break\n","\n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55DNDoFazYsI"},"source":["# Plot scores\n","plt.figure(figsize=(12,8))\n","plt.plot(final_score_log)\n","plt.grid()\n","plt.xlabel('Iteration')\n","plt.ylabel('Scores')\n","\n","plt.savefig('Score_MC.pdf', bbox_inches='tight')\n","files.download('Score_MC.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Sjw0hIkuoCG"},"source":["### Final test"]},{"cell_type":"code","metadata":{"id":"j1hlZ_CIumzm"},"source":["# Initialize the Gym environment\n","env = gym.make('MountainCar-v0') \n","env.seed(1) # Set a random seed for the environment (reproducible results)\n","\n","# This is for creating the output video in Colab, not required outside Colab\n","env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n","\n","# Let's try for a total of 10 episodes\n","for num_episode in range(10): \n","    # Reset the environment and get the initial state\n","    state = env.reset()\n","    # Reset the score. The final score will be the total amount of steps before the pole falls\n","    score = 0\n","    done = False\n","    # Go on until the pole falls off or the score reach 490\n","    while not done:\n","      # Choose the best action (temperature 0)\n","      action, q_values = choose_action_softmax(policy_net, state, temperature=0)\n","      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n","      next_state, reward, done, info = env.step(action)\n","      # Visually render the environment\n","      env.render()\n","      # Update the final score (+1 for each step)\n","      score = next_state[0] \n","      # Set the current state for the next iteration\n","      state = next_state\n","      # Check if the episode ended (the pole fell down)\n","    # Print the final score\n","    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n","env.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gs5cJtzGLvoa"},"source":["# Display the videos, not required outside Colab\n","show_videos()"],"execution_count":null,"outputs":[]}]}